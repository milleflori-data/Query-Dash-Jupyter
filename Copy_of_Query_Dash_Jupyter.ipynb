{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milleflori-data/Query-Dash-Jupyter/blob/main/Copy_of_Query_Dash_Jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸš€ Enterprise GDELT Analytics Platform\n",
        "## Professional-Grade Data Querying & Dashboard System\n",
        "\n",
        "### ğŸ¯ **Design Philosophy**\n",
        "- **Query Efficiency First**: Minimize BigQuery costs through smart caching and query optimization\n",
        "- **Professional Scalability**: Built for enterprise use with proper error handling and monitoring\n",
        "- **Desktop Deployment Ready**: Designed for standalone desktop dashboard deployment\n",
        "- **Resource Management**: Intelligent memory and API quota management\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ—ï¸ **Architecture Overview**\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    GDELT Analytics Platform                  â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚   Query Layer   â”‚  Caching Layer  â”‚    Analytics Layer      â”‚\n",
        "â”‚                 â”‚                 â”‚                         â”‚\n",
        "â”‚ â€¢ Smart Queries â”‚ â€¢ Multi-tier    â”‚ â€¢ Real-time Dashboards  â”‚\n",
        "â”‚ â€¢ Cost Control  â”‚ â€¢ Persistence   â”‚ â€¢ Export Capabilities   â”‚\n",
        "â”‚ â€¢ Monitoring    â”‚ â€¢ Validation    â”‚ â€¢ Professional Reports  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“‹ **Table of Contents**\n",
        "\n",
        "1. [ğŸ”§ Configuration & Environment](#configuration)\n",
        "2. [ğŸ’¾ Smart Caching System](#caching)\n",
        "3. [ğŸ” Optimized Query Engine](#queries)\n",
        "4. [ğŸ“Š Professional Dashboard](#dashboard)\n",
        "5. [ğŸ“ˆ Analytics & Reporting](#analytics)\n",
        "6. [ğŸ–¥ï¸ Desktop Deployment](#desktop)\n",
        "7. [ğŸš€ Performance Monitoring](#monitoring)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”§ Configuration & Environment {#configuration}\n",
        "\n",
        "### Core Dependencies\n",
        "```python\n",
        "# Core data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# BigQuery & Cloud\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import GoogleCloudError\n",
        "\n",
        "# Visualization\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.io as pio\n",
        "\n",
        "# Caching & Persistence\n",
        "import pickle\n",
        "import hashlib\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Performance & Monitoring\n",
        "import time\n",
        "import psutil\n",
        "import logging\n",
        "from functools import wraps\n",
        "```\n",
        "\n",
        "### Configuration Management\n",
        "```python\n",
        "class GDELTConfig:\n",
        "    \"\"\"Centralized configuration management\"\"\"\n",
        "\n",
        "    def __init__(self, environment=\"kaggle\"):\n",
        "        self.environment = environment\n",
        "        self.setup_config()\n",
        "\n",
        "    def setup_config(self):\n",
        "        \"\"\"Setup environment-specific configurations\"\"\"\n",
        "\n",
        "        if self.environment == \"kaggle\":\n",
        "            self.config = {\n",
        "                \"max_query_bytes\": 1_000_000_000,  # 1GB query limit\n",
        "                \"max_rows_per_query\": 100_000,\n",
        "                \"cache_dir\": \"/tmp/gdelt_cache\",\n",
        "                \"export_formats\": [\"csv\", \"json\", \"parquet\"],\n",
        "                \"visualization_theme\": \"plotly_white\"\n",
        "            }\n",
        "        elif self.environment == \"desktop\":\n",
        "            self.config = {\n",
        "                \"max_query_bytes\": 10_000_000_000,  # 10GB for desktop\n",
        "                \"max_rows_per_query\": 1_000_000,\n",
        "                \"cache_dir\": \"./gdelt_cache\",\n",
        "                \"export_formats\": [\"csv\", \"json\", \"parquet\", \"excel\"],\n",
        "                \"visualization_theme\": \"plotly_dark\"\n",
        "            }\n",
        "\n",
        "        # Create cache directory\n",
        "        Path(self.config[\"cache_dir\"]).mkdir(exist_ok=True)\n",
        "\n",
        "        # Setup logging\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(f'{self.config[\"cache_dir\"]}/gdelt.log'),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "# Initialize configuration\n",
        "config = GDELTConfig()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ’¾ Smart Caching System {#caching}\n",
        "\n",
        "### Multi-Tier Caching Strategy\n",
        "```python\n",
        "class GDELTCache:\n",
        "    \"\"\"Enterprise-grade caching system with multiple tiers\"\"\"\n",
        "\n",
        "    def __init__(self, cache_dir):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.metadata_file = self.cache_dir / \"cache_metadata.json\"\n",
        "        self.load_metadata()\n",
        "\n",
        "    def load_metadata(self):\n",
        "        \"\"\"Load cache metadata\"\"\"\n",
        "        try:\n",
        "            with open(self.metadata_file, 'r') as f:\n",
        "                self.metadata = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            self.metadata = {}\n",
        "\n",
        "    def save_metadata(self):\n",
        "        \"\"\"Save cache metadata\"\"\"\n",
        "        with open(self.metadata_file, 'w') as f:\n",
        "            json.dump(self.metadata, f, indent=2, default=str)\n",
        "\n",
        "    def generate_cache_key(self, query_params):\n",
        "        \"\"\"Generate unique cache key for query parameters\"\"\"\n",
        "        key_string = json.dumps(query_params, sort_keys=True)\n",
        "        return hashlib.md5(key_string.encode()).hexdigest()\n",
        "\n",
        "    def is_cache_valid(self, cache_key, max_age_hours=6):\n",
        "        \"\"\"Check if cached data is still valid\"\"\"\n",
        "        if cache_key not in self.metadata:\n",
        "            return False\n",
        "\n",
        "        cache_time = datetime.fromisoformat(self.metadata[cache_key]['timestamp'])\n",
        "        age = datetime.now() - cache_time\n",
        "\n",
        "        return age.total_seconds() / 3600 < max_age_hours\n",
        "\n",
        "    def cache_data(self, cache_key, data, query_params):\n",
        "        \"\"\"Cache data with metadata\"\"\"\n",
        "        cache_file = self.cache_dir / f\"{cache_key}.pkl\"\n",
        "\n",
        "        try:\n",
        "            with open(cache_file, 'wb') as f:\n",
        "                pickle.dump(data, f)\n",
        "\n",
        "            self.metadata[cache_key] = {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'rows': len(data) if hasattr(data, '__len__') else 0,\n",
        "                'query_params': query_params,\n",
        "                'file_size': cache_file.stat().st_size\n",
        "            }\n",
        "\n",
        "            self.save_metadata()\n",
        "            logging.info(f\"ğŸ“¦ Cached {len(data):,} rows to {cache_key}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"âŒ Cache write error: {e}\")\n",
        "\n",
        "    def load_cached_data(self, cache_key):\n",
        "        \"\"\"Load data from cache\"\"\"\n",
        "        cache_file = self.cache_dir / f\"{cache_key}.pkl\"\n",
        "\n",
        "        try:\n",
        "            with open(cache_file, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "\n",
        "            rows = self.metadata[cache_key]['rows']\n",
        "            logging.info(f\"ğŸ“‚ Loaded {rows:,} rows from cache\")\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"âŒ Cache read error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def cleanup_cache(self, max_size_gb=5):\n",
        "        \"\"\"Clean up old cache files\"\"\"\n",
        "        total_size = sum(f.stat().st_size for f in self.cache_dir.glob(\"*.pkl\"))\n",
        "        max_size_bytes = max_size_gb * 1024**3\n",
        "\n",
        "        if total_size > max_size_bytes:\n",
        "            # Sort by timestamp and remove oldest\n",
        "            sorted_keys = sorted(\n",
        "                self.metadata.items(),\n",
        "                key=lambda x: x[1]['timestamp']\n",
        "            )\n",
        "\n",
        "            for key, meta in sorted_keys:\n",
        "                cache_file = self.cache_dir / f\"{key}.pkl\"\n",
        "                if cache_file.exists():\n",
        "                    cache_file.unlink()\n",
        "                    del self.metadata[key]\n",
        "                    logging.info(f\"ğŸ—‘ï¸ Removed old cache: {key}\")\n",
        "\n",
        "                    total_size -= meta['file_size']\n",
        "                    if total_size <= max_size_bytes:\n",
        "                        break\n",
        "\n",
        "            self.save_metadata()\n",
        "\n",
        "# Initialize cache system\n",
        "cache_system = GDELTCache(config.config[\"cache_dir\"])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ” Optimized Query Engine {#queries}\n",
        "\n",
        "### Smart Query Builder with Cost Control\n",
        "```python\n",
        "class GDELTQueryEngine:\n",
        "    \"\"\"Enterprise query engine with cost optimization\"\"\"\n",
        "\n",
        "    def __init__(self, client, cache_system):\n",
        "        self.client = client\n",
        "        self.cache = cache_system\n",
        "        self.query_stats = {\"total_queries\": 0, \"total_bytes\": 0, \"cache_hits\": 0}\n",
        "\n",
        "    def estimate_query_cost(self, query):\n",
        "        \"\"\"Estimate BigQuery cost before execution\"\"\"\n",
        "        if not self.client:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=True)\n",
        "            query_job = self.client.query(query, job_config=job_config)\n",
        "\n",
        "            bytes_processed = query_job.total_bytes_processed\n",
        "            estimated_cost = (bytes_processed / 1024**4) * 5  # $5 per TB\n",
        "\n",
        "            return {\n",
        "                \"bytes_processed\": bytes_processed,\n",
        "                \"estimated_cost_usd\": estimated_cost,\n",
        "                \"size_mb\": bytes_processed / 1024**2\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"âŒ Cost estimation failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def build_optimized_query(self, params):\n",
        "        \"\"\"Build highly optimized GDELT query\"\"\"\n",
        "\n",
        "        # Smart date partitioning\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=params.get('days_back', 30))\n",
        "\n",
        "        # Use YYYYMMDD format for partition pruning\n",
        "        start_str = start_date.strftime('%Y%m%d')\n",
        "        end_str = end_date.strftime('%Y%m%d')\n",
        "\n",
        "        # Build efficient query with proper indexing\n",
        "        query = f\"\"\"\n",
        "        WITH filtered_events AS (\n",
        "            SELECT\n",
        "                SQLDATE,\n",
        "                Actor1Name,\n",
        "                Actor2Name,\n",
        "                EventCode,\n",
        "                EventRootCode,\n",
        "                CAST(EventCode AS STRING) as EventCodeStr,\n",
        "                QuadClass,\n",
        "                GoldsteinScale,\n",
        "                NumMentions,\n",
        "                AvgTone,\n",
        "                ActionGeo_CountryCode,\n",
        "                ActionGeo_Lat,\n",
        "                ActionGeo_Long,\n",
        "                SOURCEURL\n",
        "            FROM `gdelt-bq.gdeltv2.events`\n",
        "            WHERE\n",
        "                -- Partition pruning for cost efficiency\n",
        "                _PARTITIONTIME >= TIMESTAMP('{start_date.strftime('%Y-%m-%d')}')\n",
        "                AND _PARTITIONTIME <= TIMESTAMP('{end_date.strftime('%Y-%m-%d')}')\n",
        "                AND SQLDATE >= {start_str}\n",
        "                AND SQLDATE <= {end_str}\n",
        "                -- Quality filters\n",
        "                AND Actor1Name IS NOT NULL\n",
        "                AND EventCode IS NOT NULL\n",
        "                AND NumMentions >= {params.get('min_mentions', 5)}\n",
        "                AND ActionGeo_CountryCode IS NOT NULL\n",
        "                AND LENGTH(Actor1Name) < 100  -- Filter out URLs/garbage\n",
        "        \"\"\"\n",
        "\n",
        "        # Add country filter with efficient IN clause\n",
        "        if params.get('countries'):\n",
        "            countries_str = \"', '\".join(params['countries'])\n",
        "            query += f\"\\n                AND ActionGeo_CountryCode IN ('{countries_str}')\"\n",
        "\n",
        "        # Add category filter using event codes\n",
        "        if params.get('categories'):\n",
        "            event_codes = self._get_event_codes_for_categories(params['categories'])\n",
        "            if event_codes:\n",
        "                codes_str = \"', '\".join(event_codes)\n",
        "                query += f\"\\n                AND EventRootCode IN ('{codes_str}')\"\n",
        "\n",
        "        # Add tone filter for sentiment analysis\n",
        "        if params.get('tone_range'):\n",
        "            min_tone, max_tone = params['tone_range']\n",
        "            query += f\"\\n                AND AvgTone BETWEEN {min_tone} AND {max_tone}\"\n",
        "\n",
        "        query += f\"\"\"\n",
        "        ),\n",
        "        ranked_events AS (\n",
        "            SELECT *,\n",
        "                ROW_NUMBER() OVER (\n",
        "                    ORDER BY NumMentions DESC, SQLDATE DESC\n",
        "                ) as rn\n",
        "            FROM filtered_events\n",
        "        )\n",
        "        SELECT * EXCEPT(rn)\n",
        "        FROM ranked_events\n",
        "        WHERE rn <= {params.get('limit', 100000)}\n",
        "        \"\"\"\n",
        "\n",
        "        return query\n",
        "\n",
        "    def _get_event_codes_for_categories(self, categories):\n",
        "        \"\"\"Enhanced category to event code mapping\"\"\"\n",
        "        category_codes = {\n",
        "            \"Politics / Diplomacy\": ['01', '02', '03', '04', '05'],\n",
        "            \"Military / Conflict\": ['13', '14'],\n",
        "            \"Finance / Markets\": ['07'],\n",
        "            \"Health / Medical\": ['042', '043', '044'],\n",
        "            \"Technology / Science\": ['081', '082', '083'],\n",
        "            \"Environment / Weather\": ['045', '087', '088'],\n",
        "            \"Unconventional Violence\": ['15', '16', '17'],\n",
        "            \"Mass Violence\": ['18', '19', '20'],\n",
        "            \"Social / Cultural\": ['06', '08', '09'],\n",
        "            \"Legal / Judicial\": ['10', '11', '12']\n",
        "        }\n",
        "\n",
        "        all_codes = []\n",
        "        for cat in categories:\n",
        "            if cat in category_codes:\n",
        "                all_codes.extend(category_codes[cat])\n",
        "\n",
        "        return list(set(all_codes))  # Remove duplicates\n",
        "\n",
        "    @wraps\n",
        "    def execute_query(self, params, use_cache=True, max_cost_usd=1.0):\n",
        "        \"\"\"Execute query with caching and cost control\"\"\"\n",
        "\n",
        "        # Check cache first\n",
        "        cache_key = self.cache.generate_cache_key(params)\n",
        "\n",
        "        if use_cache and self.cache.is_cache_valid(cache_key):\n",
        "            self.query_stats[\"cache_hits\"] += 1\n",
        "            return self.cache.load_cached_data(cache_key)\n",
        "\n",
        "        # Build and validate query\n",
        "        query = self.build_optimized_query(params)\n",
        "\n",
        "        # Cost estimation\n",
        "        cost_estimate = self.estimate_query_cost(query)\n",
        "        if cost_estimate:\n",
        "            logging.info(f\"ğŸ’° Query will process {cost_estimate['size_mb']:.1f} MB\")\n",
        "            logging.info(f\"ğŸ’° Estimated cost: ${cost_estimate['estimated_cost_usd']:.4f}\")\n",
        "\n",
        "            if cost_estimate['estimated_cost_usd'] > max_cost_usd:\n",
        "                raise ValueError(f\"Query cost ${cost_estimate['estimated_cost_usd']:.4f} exceeds limit ${max_cost_usd}\")\n",
        "\n",
        "        # Execute query with monitoring\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            logging.info(f\"ğŸ”„ Executing BigQuery...\")\n",
        "            query_job = self.client.query(query)\n",
        "            df = query_job.to_dataframe()\n",
        "\n",
        "            execution_time = time.time() - start_time\n",
        "\n",
        "            # Update statistics\n",
        "            self.query_stats[\"total_queries\"] += 1\n",
        "            if cost_estimate:\n",
        "                self.query_stats[\"total_bytes\"] += cost_estimate[\"bytes_processed\"]\n",
        "\n",
        "            logging.info(f\"âœ… Query completed in {execution_time:.2f}s - {len(df):,} rows\")\n",
        "\n",
        "            # Cache results\n",
        "            if use_cache:\n",
        "                self.cache.cache_data(cache_key, df, params)\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"âŒ Query execution failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_query_statistics(self):\n",
        "        \"\"\"Get comprehensive query statistics\"\"\"\n",
        "        total_cost = (self.query_stats[\"total_bytes\"] / 1024**4) * 5\n",
        "\n",
        "        return {\n",
        "            \"total_queries\": self.query_stats[\"total_queries\"],\n",
        "            \"cache_hits\": self.query_stats[\"cache_hits\"],\n",
        "            \"cache_hit_rate\": self.query_stats[\"cache_hits\"] / max(self.query_stats[\"total_queries\"], 1),\n",
        "            \"total_bytes_processed\": self.query_stats[\"total_bytes\"],\n",
        "            \"estimated_total_cost\": total_cost,\n",
        "            \"avg_cost_per_query\": total_cost / max(self.query_stats[\"total_queries\"], 1)\n",
        "        }\n",
        "\n",
        "# Initialize query engine\n",
        "query_engine = GDELTQueryEngine(client, cache_system) if 'client' in globals() else None\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Š Professional Dashboard System {#dashboard}\n",
        "\n",
        "### Enterprise Dashboard Builder\n",
        "```python\n",
        "class GDELTProfessionalDashboard:\n",
        "    \"\"\"Professional-grade dashboard with enterprise features\"\"\"\n",
        "\n",
        "    def __init__(self, theme=\"plotly_white\"):\n",
        "        self.theme = theme\n",
        "        pio.templates.default = theme\n",
        "\n",
        "        # Professional color schemes\n",
        "        self.colors = {\n",
        "            \"primary\": \"#2E86C1\",\n",
        "            \"secondary\": \"#F39C12\",\n",
        "            \"success\": \"#27AE60\",\n",
        "            \"warning\": \"#E74C3C\",\n",
        "            \"neutral\": \"#95A5A6\",\n",
        "            \"dark\": \"#34495E\"\n",
        "        }\n",
        "\n",
        "    def create_executive_summary(self, df, params):\n",
        "        \"\"\"Create executive summary with KPIs\"\"\"\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            return None\n",
        "\n",
        "        # Calculate KPIs\n",
        "        total_events = len(df)\n",
        "        date_range = (df['date'].min(), df['date'].max())\n",
        "        avg_tone = df['AvgTone'].mean()\n",
        "        top_actor = df.groupby('Actor1Name')['NumMentions'].sum().idxmax()\n",
        "        most_active_country = df['ActionGeo_CountryCode'].value_counts().index[0]\n",
        "\n",
        "        # Create KPI cards\n",
        "        kpi_fig = go.Figure()\n",
        "\n",
        "        kpi_cards = [\n",
        "            {\"title\": \"Total Events\", \"value\": f\"{total_events:,}\", \"color\": self.colors[\"primary\"]},\n",
        "            {\"title\": \"Date Range\", \"value\": f\"{date_range[0].strftime('%m/%d')} - {date_range[1].strftime('%m/%d')}\", \"color\": self.colors[\"secondary\"]},\n",
        "            {\"title\": \"Avg Sentiment\", \"value\": f\"{avg_tone:.2f}\", \"color\": self.colors[\"success\"] if avg_tone > 0 else self.colors[\"warning\"]},\n",
        "            {\"title\": \"Top Actor\", \"value\": str(top_actor)[:20], \"color\": self.colors[\"dark\"]},\n",
        "            {\"title\": \"Most Active\", \"value\": most_active_country, \"color\": self.colors[\"neutral\"]}\n",
        "        ]\n",
        "\n",
        "        # Add annotations for KPIs\n",
        "        annotations = []\n",
        "        for i, kpi in enumerate(kpi_cards):\n",
        "            x_pos = (i + 1) / (len(kpi_cards) + 1)\n",
        "\n",
        "            annotations.extend([\n",
        "                dict(x=x_pos, y=0.7, text=f\"<b>{kpi['value']}</b>\",\n",
        "                     showarrow=False, font=dict(size=24, color=kpi['color'])),\n",
        "                dict(x=x_pos, y=0.3, text=kpi['title'],\n",
        "                     showarrow=False, font=dict(size=14, color=\"gray\"))\n",
        "            ])\n",
        "\n",
        "        kpi_fig.update_layout(\n",
        "            title=\"ğŸ“Š Executive Summary\",\n",
        "            height=200,\n",
        "            showlegend=False,\n",
        "            xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
        "            yaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
        "            annotations=annotations,\n",
        "            plot_bgcolor=\"rgba(0,0,0,0)\"\n",
        "        )\n",
        "\n",
        "        return kpi_fig\n",
        "\n",
        "    def create_comprehensive_dashboard(self, df, params):\n",
        "        \"\"\"Create comprehensive multi-panel dashboard\"\"\"\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            return None\n",
        "\n",
        "        # Process data for visualizations\n",
        "        timeline_data = self._prepare_timeline_data(df)\n",
        "        actor_data = self._prepare_actor_data(df)\n",
        "        geo_data = self._prepare_geo_data(df)\n",
        "        sentiment_data = self._prepare_sentiment_data(df)\n",
        "        category_data = self._prepare_category_data(df)\n",
        "\n",
        "        # Create subplot structure\n",
        "        fig = make_subplots(\n",
        "            rows=3, cols=3,\n",
        "            subplot_titles=[\n",
        "                \"ğŸ“ˆ Event Timeline\", \"ğŸ­ Top Actors by Influence\", \"ğŸŒ Geographic Distribution\",\n",
        "                \"ğŸ˜Š Sentiment Analysis\", \"ğŸ“Š Event Categories\", \"âš¡ Event Intensity Heatmap\",\n",
        "                \"ğŸ”— Actor Network\", \"ğŸ“… Weekly Patterns\", \"ğŸ¯ Key Metrics\"\n",
        "            ],\n",
        "            specs=[\n",
        "                [{\"type\": \"scatter\"}, {\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
        "                [{\"type\": \"histogram\"}, {\"type\": \"pie\"}, {\"type\": \"heatmap\"}],\n",
        "                [{\"type\": \"scatter\"}, {\"type\": \"bar\"}, {\"type\": \"table\"}]\n",
        "            ],\n",
        "            horizontal_spacing=0.08,\n",
        "            vertical_spacing=0.1\n",
        "        )\n",
        "\n",
        "        # 1. Timeline with trend analysis\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=timeline_data['date'],\n",
        "                y=timeline_data['events'],\n",
        "                mode='lines+markers',\n",
        "                name='Daily Events',\n",
        "                line=dict(color=self.colors[\"primary\"], width=3),\n",
        "                hovertemplate='<b>%{x}</b><br>Events: %{y}<extra></extra>'\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # Add trend line\n",
        "        z = np.polyfit(range(len(timeline_data)), timeline_data['events'], 1)\n",
        "        trend = np.poly1d(z)(range(len(timeline_data)))\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=timeline_data['date'],\n",
        "                y=trend,\n",
        "                mode='lines',\n",
        "                name='Trend',\n",
        "                line=dict(color=self.colors[\"warning\"], width=2, dash='dash')\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # 2. Top Actors with enhanced styling\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=actor_data['total_mentions'][:10],\n",
        "                y=actor_data['Actor1Name'][:10],\n",
        "                orientation='h',\n",
        "                name='Actor Influence',\n",
        "                marker_color=self.colors[\"secondary\"],\n",
        "                text=actor_data['total_mentions'][:10],\n",
        "                textposition='auto',\n",
        "                hovertemplate='<b>%{y}</b><br>Total Mentions: %{x:,}<extra></extra>'\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # 3. Geographic scatter plot\n",
        "        fig.add_trace(\n",
        "            go.Scattermap(\n",
        "                lat=geo_data['ActionGeo_Lat'],\n",
        "                lon=geo_data['ActionGeo_Long'],\n",
        "                mode='markers',\n",
        "                marker=dict(\n",
        "                    size=geo_data['events'],\n",
        "                    sizemode='diameter',\n",
        "                    sizeref=geo_data['events'].max()/50,\n",
        "                    color=geo_data['avg_tone'],\n",
        "                    colorscale='RdYlBu',\n",
        "                    colorbar=dict(title=\"Avg Tone\"),\n",
        "                    line=dict(color='black', width=1)\n",
        "                ),\n",
        "                text=geo_data['ActionGeo_CountryCode'],\n",
        "                hovertemplate='<b>%{text}</b><br>Events: %{marker.size}<br>Avg Tone: %{marker.color:.2f}<extra></extra>'\n",
        "            ),\n",
        "            row=1, col=3\n",
        "        )\n",
        "\n",
        "        # 4. Sentiment histogram\n",
        "        fig.add_trace(\n",
        "            go.Histogram(\n",
        "                x=sentiment_data['AvgTone'],\n",
        "                nbinsx=30,\n",
        "                name='Sentiment Distribution',\n",
        "                marker_color=self.colors[\"success\"],\n",
        "                opacity=0.7\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # 5. Category pie chart\n",
        "        fig.add_trace(\n",
        "            go.Pie(\n",
        "                labels=category_data['Category'],\n",
        "                values=category_data['Events'],\n",
        "                name='Categories',\n",
        "                hole=0.4,\n",
        "                hovertemplate='<b>%{label}</b><br>Events: %{value}<br>Percentage: %{percent}<extra></extra>'\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        # Continue with remaining visualizations...\n",
        "\n",
        "        # Update layout with professional styling\n",
        "        fig.update_layout(\n",
        "            title={\n",
        "                'text': f\"ğŸŒ GDELT Professional Analytics Dashboard<br><sup>Generated on {datetime.now().strftime('%Y-%m-%d %H:%M UTC')} | {len(df):,} Events Analyzed</sup>\",\n",
        "                'x': 0.5,\n",
        "                'xanchor': 'center',\n",
        "                'font': {'size': 20}\n",
        "            },\n",
        "            height=1200,\n",
        "            showlegend=False,\n",
        "            template=self.theme,\n",
        "            font=dict(family=\"Arial, sans-serif\")\n",
        "        )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def _prepare_timeline_data(self, df):\n",
        "        \"\"\"Prepare timeline data with aggregations\"\"\"\n",
        "        return df.groupby('date').agg({\n",
        "            'NumMentions': ['count', 'sum', 'mean'],\n",
        "            'AvgTone': 'mean'\n",
        "        }).round(2).reset_index()\n",
        "\n",
        "    def _prepare_actor_data(self, df):\n",
        "        \"\"\"Prepare actor data with influence metrics\"\"\"\n",
        "        return (df.groupby('Actor1Name')\n",
        "                .agg({\n",
        "                    'NumMentions': ['sum', 'count', 'mean'],\n",
        "                    'AvgTone': 'mean'\n",
        "                })\n",
        "                .round(2)\n",
        "                .sort_values(('NumMentions', 'sum'), ascending=False)\n",
        "                .reset_index())\n",
        "\n",
        "    def _prepare_geo_data(self, df):\n",
        "        \"\"\"Prepare geographic data with coordinates\"\"\"\n",
        "        geo_df = df.dropna(subset=['ActionGeo_Lat', 'ActionGeo_Long'])\n",
        "        return (geo_df.groupby(['ActionGeo_CountryCode', 'ActionGeo_Lat', 'ActionGeo_Long'])\n",
        "                .agg({\n",
        "                    'NumMentions': ['count', 'sum'],\n",
        "                    'AvgTone': 'mean'\n",
        "                })\n",
        "                .round(2)\n",
        "                .reset_index())\n",
        "\n",
        "    def _prepare_sentiment_data(self, df):\n",
        "        \"\"\"Prepare sentiment analysis data\"\"\"\n",
        "        return df.dropna(subset=['AvgTone'])\n",
        "\n",
        "    def _prepare_category_data(self, df):\n",
        "        \"\"\"Prepare category distribution data\"\"\"\n",
        "        return df['Category'].value_counts().reset_index()\n",
        "\n",
        "# Initialize dashboard system\n",
        "dashboard = GDELTProfessionalDashboard()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ–¥ï¸ Desktop Deployment Strategy {#desktop}\n",
        "\n",
        "### Standalone Desktop Application\n",
        "```python\n",
        "class GDELTDesktopApp:\n",
        "    \"\"\"Desktop deployment with local server capabilities\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.config = GDELTConfig(\"desktop\")\n",
        "        self.setup_desktop_environment()\n",
        "\n",
        "    def setup_desktop_environment(self):\n",
        "        \"\"\"Setup desktop-specific configurations\"\"\"\n",
        "\n",
        "        # Enhanced local storage\n",
        "        self.data_dir = Path.home() / \"GDELTAnalytics\"\n",
        "        self.data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Local database option (SQLite for offline work)\n",
        "        self.local_db = self.data_dir / \"gdelt_local.db\"\n",
        "\n",
        "        # Desktop-specific features\n",
        "        self.features = {\n",
        "            \"offline_mode\": True,\n",
        "            \"local_storage\": True,\n",
        "            \"export_options\": [\"pdf\", \"pptx\", \"excel\", \"csv\", \"json\"],\n",
        "            \"scheduled_updates\": True,\n",
        "            \"email_reports\": True\n",
        "        }\n",
        "\n",
        "    def create_desktop_launcher(self):\n",
        "        \"\"\"Create desktop application launcher\"\"\"\n",
        "\n",
        "        launcher_script = f\"\"\"\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "import webbrowser\n",
        "from threading import Timer\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Change to application directory\n",
        "os.chdir('{self.data_dir}')\n",
        "\n",
        "# Start Jupyter server with custom settings\n",
        "jupyter_cmd = [\n",
        "    sys.executable, '-m', 'jupyter', 'notebook',\n",
        "    '--no-browser',\n",
        "    '--port=8888',\n",
        "    '--NotebookApp.token=\"\"',\n",
        "    '--NotebookApp.password=\"\"',\n",
        "    f'--notebook-dir={self.data_dir}'\n",
        "]\n",
        "\n",
        "# Start server\n",
        "process = subprocess.Popen(jupyter_cmd)\n",
        "\n",
        "# Open browser after delay\n",
        "Timer(3, lambda: webbrowser.open('http://localhost:8888')).start()\n",
        "\n",
        "print(\"ğŸš€ GDELT Analytics Desktop launched!\")\n",
        "print(\"ğŸ“Š Dashboard available at: http://localhost:8888\")\n",
        "print(\"Press Ctrl+C to stop the server\")\n",
        "\n",
        "try:\n",
        "    process.wait()\n",
        "except KeyboardInterrupt:\n",
        "    process.terminate()\n",
        "    print(\"\\\\nâœ… GDELT Analytics stopped\")\n",
        "\"\"\"\n",
        "\n",
        "        launcher_path = self.data_dir / \"launch_gdelt.py\"\n",
        "        launcher_path.write_text(launcher_script)\n",
        "        launcher_path.chmod(0o755)\n",
        "\n",
        "        return launcher_path\n",
        "\n",
        "    def setup_offline_capabilities(self):\n",
        "        \"\"\"Setup offline data storage and processing\"\"\"\n",
        "\n",
        "        # SQLite schema for local storage\n",
        "        import sqlite3\n",
        "\n",
        "        conn = sqlite3.connect(self.local_db)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS gdelt_events (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            sqldate INTEGER,\n",
        "            actor1_name TEXT,\n",
        "            actor2_name TEXT,\n",
        "            event_code TEXT,\n",
        "            event_root_code TEXT,\n",
        "            quad_class INTEGER,\n",
        "            goldstein_scale REAL,\n",
        "            num_mentions INTEGER,\n",
        "            avg_tone REAL,\n",
        "            country_code TEXT,\n",
        "            latitude REAL,\n",
        "            longitude REAL,\n",
        "            source_url TEXT,\n",
        "            category TEXT,\n",
        "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "        )\n",
        "        \"\"\")\n",
        "\n",
        "        cursor.execute(\"\"\"\n",
        "        CREATE INDEX IF NOT EXISTS idx_date ON gdelt_events(sqldate);\n",
        "        CREATE INDEX IF NOT EXISTS idx_country ON gdelt_events(country_code);\n",
        "        CREATE INDEX IF NOT EXISTS idx_category ON gdelt_events(category);\n",
        "        CREATE INDEX IF NOT EXISTS idx_actor ON gdelt_events(actor1_name);\n",
        "        \"\"\")\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "        return self.local_db\n",
        "\n",
        "    def create_scheduled_updater(self):\n",
        "        \"\"\"Create scheduled data update system\"\"\"\n",
        "\n",
        "        update_script = f\"\"\"\n",
        "import schedule\n",
        "import time\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    filename='{self.data_dir}/updater.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(message)s'\n",
        ")\n",
        "\n",
        "def update_gdelt_data():\n",
        "    \\\"\\\"\\\"Scheduled data update function\\\"\\\"\\\"\n",
        "    try:\n",
        "        logging.info(\"ğŸ”„ Starting scheduled GDELT data update\")\n",
        "\n",
        "        # Import and run update logic here\n",
        "        # This would call your query engine with fresh data\n",
        "\n",
        "        logging.info(\"âœ… Scheduled update completed successfully\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-09T16:16:50.694662Z",
          "iopub.execute_input": "2025-09-09T16:16:50.69495Z",
          "iopub.status.idle": "2025-09-09T16:16:50.743294Z",
          "shell.execute_reply.started": "2025-09-09T16:16:50.694927Z",
          "shell.execute_reply": "2025-09-09T16:16:50.742133Z"
        },
        "id": "_rLZtMZIS6tU",
        "outputId": "422c7a69-d85f-4463-90a0-0d7996966d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character 'â”Œ' (U+250C) (3860803288.py, line 15)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_70/3860803288.py\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character 'â”Œ' (U+250C)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jbtigUOmS6tW"
      }
    }
  ]
}